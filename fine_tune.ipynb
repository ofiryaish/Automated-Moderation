{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "! pip install -q -U bitsandbytes\n",
    "! pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "! pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "! pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "! pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# You only need to run this once per machine\n",
    "! pip install -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was getting error about the GPU driver that need to be updated, so I downgrated the torch version\n",
    "! pip uninstall torch\n",
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_Mar__8_18:18:20_PST_2022\n",
      "Cuda compilation tools, release 11.6, V11.6.124\n",
      "Build cuda_11.6.r11.6/compiler.31057947_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.287, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016366243362426758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc8b116a642497cbd61f77565594225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tables\n",
    "conversation_df = pd.read_csv(\"annotated_trees_101.csv\")\n",
    "conversation_df = conversation_df.rename({'Unnamed: 0': \"index\"}, axis=1)\n",
    "df_only_bad_tone = pd.read_csv(\"bad_tone_nodes_with_generated_messages_chat_gpt_3_5_turbo.csv\")\n",
    "df_only_bad_tone[\"neg branch path\"] = df_only_bad_tone[\"neg branch path\"].apply(eval) # converts the list string into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train dataset\n",
    "train_data = list()\n",
    "for i in range(df_only_bad_tone.shape[0]):\n",
    "    index = df_only_bad_tone[\"index\"].iloc[i]\n",
    "    prompt = utilities.generate_branch_for_negative_tone_prompt_for_mistral(df_only_bad_tone, conversation_df, node_index=i)\n",
    "    label = df_only_bad_tone[\"generated_moderation\"].iloc[i]\n",
    "    train_data.append((index, prompt, label))\n",
    "\n",
    "dataset_df = pd.DataFrame(train_data, columns=[\"index\", 'prompt', 'label'])\n",
    "dataset_df.to_pickle(\"train_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01874852180480957,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0121f5a62074dd3aa3cb48e46ba3483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'prompt', 'label'],\n",
       "    num_rows: 1732\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "mod_dataset = load_dataset(\"pandas\", data_files=\"train_dataset.pkl\", split='train')\n",
    "mod_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013004302978515625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 1732,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64a537d92464dfaa326785e8e9bb600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_mod_prompt(data_point):\n",
    "    return f\"\"\"{data_point['prompt']}\\n{data_point['label']}</s>\"\"\"\n",
    "\n",
    "\n",
    "text_column = [generate_mod_prompt(ex) for ex in mod_dataset]\n",
    "mod_dataset = mod_dataset.add_column(\"full_prompt\", text_column)\n",
    "\n",
    "mod_dataset = mod_dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "mod_dataset = mod_dataset.map(lambda samples: tokenizer(samples[\"full_prompt\"]), batched=True)\n",
    "mod_dataset = mod_dataset.train_test_split(test_size=0.2, seed=1234)\n",
    "train_data = mod_dataset[\"train\"]\n",
    "test_data = mod_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'prompt', 'label', 'full_prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1385\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'prompt', 'label', 'full_prompt', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 347\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'gate_proj', 'down_proj', 'up_proj', 'q_proj', 'o_proj', 'k_proj']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 20971520 | total: 7262703616 | Percentage: 0.2888%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.287, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#new code using SFTTrainer\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"full_prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=1,\n",
    "        max_steps=100,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=1,\n",
    "        save_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        output_dir=\"outputs_mine\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        bf16=True,\n",
    "        \n",
    "        logging_dir=\"./logs_mine\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 48:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.799100</td>\n",
       "      <td>1.808764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.546500</td>\n",
       "      <td>1.755077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.702700</td>\n",
       "      <td>1.713680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.547100</td>\n",
       "      <td>1.672189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.696000</td>\n",
       "      <td>1.630933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.460600</td>\n",
       "      <td>1.593421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.529400</td>\n",
       "      <td>1.561854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.563400</td>\n",
       "      <td>1.539072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.548600</td>\n",
       "      <td>1.524203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.588400</td>\n",
       "      <td>1.518224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.6634824967384338, metrics={'train_runtime': 2940.5416, 'train_samples_per_second': 0.272, 'train_steps_per_second': 0.034, 'total_flos': 3.4846549875572736e+16, 'train_loss': 1.6634824967384338, 'epoch': 0.58})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load fined tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013045072555541992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0725bdd1b82440979b0ce650ab73c8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"outputs_mine/checkpoint-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model, tokenizer):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=300, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    \n",
    "    return decoded[0]\n",
    "\n",
    "def get_mod_message_generation_function(model, tokenizer):\n",
    "    def generate_moderation_message(row):\n",
    "        print('sample:', row.name)\n",
    "        answer = get_completion(row['prompt'], model=model, tokenizer=tokenizer)\n",
    "        splits = answer.split('[/INST]')\n",
    "        if len(splits) != 2:\n",
    "            return 'model produced illegal output..'\n",
    "        return splits[-1].rstrip('</s>')\n",
    "    \n",
    "    return generate_moderation_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_data.to_pandas(), test_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 0\n",
      "sample: 1\n",
      "sample: 2\n",
      "sample: 3\n",
      "sample: 4\n",
      "sample: 5\n",
      "sample: 6\n",
      "sample: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 8\n",
      "sample: 9\n",
      "sample: 10\n",
      "sample: 11\n",
      "sample: 12\n",
      "sample: 13\n",
      "sample: 14\n",
      "sample: 15\n",
      "sample: 16\n",
      "sample: 17\n",
      "sample: 18\n",
      "sample: 19\n",
      "sample: 20\n",
      "sample: 21\n",
      "sample: 22\n",
      "sample: 23\n",
      "sample: 24\n",
      "sample: 25\n",
      "sample: 26\n",
      "sample: 27\n",
      "sample: 28\n",
      "sample: 29\n",
      "sample: 30\n",
      "sample: 31\n",
      "sample: 32\n",
      "sample: 33\n",
      "sample: 34\n",
      "sample: 35\n",
      "sample: 36\n",
      "sample: 37\n",
      "sample: 38\n",
      "sample: 39\n",
      "sample: 40\n",
      "sample: 41\n",
      "sample: 42\n",
      "sample: 43\n",
      "sample: 44\n",
      "sample: 45\n",
      "sample: 46\n",
      "sample: 47\n",
      "sample: 48\n",
      "sample: 49\n",
      "sample: 50\n",
      "sample: 51\n",
      "sample: 52\n",
      "sample: 53\n",
      "sample: 54\n",
      "sample: 55\n",
      "sample: 56\n",
      "sample: 57\n",
      "sample: 58\n",
      "sample: 59\n",
      "sample: 60\n",
      "sample: 61\n",
      "sample: 62\n",
      "sample: 63\n",
      "sample: 64\n",
      "sample: 65\n",
      "sample: 66\n",
      "sample: 67\n",
      "sample: 68\n",
      "sample: 69\n",
      "sample: 70\n",
      "sample: 71\n",
      "sample: 72\n",
      "sample: 73\n",
      "sample: 74\n",
      "sample: 75\n",
      "sample: 76\n",
      "sample: 77\n",
      "sample: 78\n",
      "sample: 79\n",
      "sample: 80\n",
      "sample: 81\n",
      "sample: 82\n",
      "sample: 83\n",
      "sample: 84\n",
      "sample: 85\n",
      "sample: 86\n",
      "sample: 87\n",
      "sample: 88\n",
      "sample: 89\n",
      "sample: 90\n",
      "sample: 91\n",
      "sample: 92\n",
      "sample: 93\n",
      "sample: 94\n",
      "sample: 95\n",
      "sample: 96\n",
      "sample: 97\n",
      "sample: 98\n",
      "sample: 99\n",
      "sample: 100\n",
      "sample: 101\n",
      "sample: 102\n",
      "sample: 103\n",
      "sample: 104\n",
      "sample: 105\n",
      "sample: 106\n",
      "sample: 107\n",
      "sample: 108\n",
      "sample: 109\n",
      "sample: 110\n",
      "sample: 111\n",
      "sample: 112\n",
      "sample: 113\n",
      "sample: 114\n",
      "sample: 115\n",
      "sample: 116\n",
      "sample: 117\n",
      "sample: 118\n",
      "sample: 119\n",
      "sample: 120\n",
      "sample: 121\n",
      "sample: 122\n",
      "sample: 123\n",
      "sample: 124\n",
      "sample: 125\n",
      "sample: 126\n",
      "sample: 127\n",
      "sample: 128\n",
      "sample: 129\n",
      "sample: 130\n",
      "sample: 131\n",
      "sample: 132\n",
      "sample: 133\n",
      "sample: 134\n",
      "sample: 135\n",
      "sample: 136\n",
      "sample: 137\n",
      "sample: 138\n",
      "sample: 139\n",
      "sample: 140\n",
      "sample: 141\n",
      "sample: 142\n",
      "sample: 143\n",
      "sample: 144\n",
      "sample: 145\n",
      "sample: 146\n",
      "sample: 147\n",
      "sample: 148\n",
      "sample: 149\n",
      "sample: 150\n",
      "sample: 151\n",
      "sample: 152\n",
      "sample: 153\n",
      "sample: 154\n",
      "sample: 155\n",
      "sample: 156\n",
      "sample: 157\n",
      "sample: 158\n",
      "sample: 159\n",
      "sample: 160\n",
      "sample: 161\n",
      "sample: 162\n",
      "sample: 163\n",
      "sample: 164\n",
      "sample: 165\n",
      "sample: 166\n",
      "sample: 167\n",
      "sample: 168\n",
      "sample: 169\n",
      "sample: 170\n",
      "sample: 171\n",
      "sample: 172\n",
      "sample: 173\n",
      "sample: 174\n",
      "sample: 175\n",
      "sample: 176\n",
      "sample: 177\n",
      "sample: 178\n",
      "sample: 179\n",
      "sample: 180\n",
      "sample: 181\n",
      "sample: 182\n",
      "sample: 183\n",
      "sample: 184\n",
      "sample: 185\n",
      "sample: 186\n",
      "sample: 187\n",
      "sample: 188\n",
      "sample: 189\n",
      "sample: 190\n",
      "sample: 191\n",
      "sample: 192\n",
      "sample: 193\n",
      "sample: 194\n",
      "sample: 195\n",
      "sample: 196\n",
      "sample: 197\n",
      "sample: 198\n",
      "sample: 199\n",
      "sample: 200\n",
      "sample: 201\n",
      "sample: 202\n",
      "sample: 203\n",
      "sample: 204\n",
      "sample: 205\n",
      "sample: 206\n",
      "sample: 207\n",
      "sample: 208\n",
      "sample: 209\n",
      "sample: 210\n",
      "sample: 211\n",
      "sample: 212\n",
      "sample: 213\n",
      "sample: 214\n",
      "sample: 215\n",
      "sample: 216\n",
      "sample: 217\n",
      "sample: 218\n",
      "sample: 219\n",
      "sample: 220\n",
      "sample: 221\n",
      "sample: 222\n",
      "sample: 223\n",
      "sample: 224\n",
      "sample: 225\n",
      "sample: 226\n",
      "sample: 227\n",
      "sample: 228\n",
      "sample: 229\n",
      "sample: 230\n",
      "sample: 231\n",
      "sample: 232\n",
      "sample: 233\n",
      "sample: 234\n",
      "sample: 235\n",
      "sample: 236\n",
      "sample: 237\n",
      "sample: 238\n",
      "sample: 239\n",
      "sample: 240\n",
      "sample: 241\n",
      "sample: 242\n",
      "sample: 243\n",
      "sample: 244\n",
      "sample: 245\n",
      "sample: 246\n",
      "sample: 247\n",
      "sample: 248\n",
      "sample: 249\n",
      "sample: 250\n",
      "sample: 251\n",
      "sample: 252\n",
      "sample: 253\n",
      "sample: 254\n",
      "sample: 255\n",
      "sample: 256\n",
      "sample: 257\n",
      "sample: 258\n",
      "sample: 259\n",
      "sample: 260\n",
      "sample: 261\n",
      "sample: 262\n",
      "sample: 263\n",
      "sample: 264\n",
      "sample: 265\n",
      "sample: 266\n",
      "sample: 267\n",
      "sample: 268\n",
      "sample: 269\n",
      "sample: 270\n",
      "sample: 271\n",
      "sample: 272\n",
      "sample: 273\n",
      "sample: 274\n",
      "sample: 275\n",
      "sample: 276\n",
      "sample: 277\n",
      "sample: 278\n",
      "sample: 279\n",
      "sample: 280\n",
      "sample: 281\n",
      "sample: 282\n",
      "sample: 283\n",
      "sample: 284\n",
      "sample: 285\n",
      "sample: 286\n",
      "sample: 287\n",
      "sample: 288\n",
      "sample: 289\n",
      "sample: 290\n",
      "sample: 291\n",
      "sample: 292\n",
      "sample: 293\n",
      "sample: 294\n",
      "sample: 295\n",
      "sample: 296\n",
      "sample: 297\n",
      "sample: 298\n",
      "sample: 299\n",
      "sample: 300\n",
      "sample: 301\n",
      "sample: 302\n",
      "sample: 303\n",
      "sample: 304\n",
      "sample: 305\n",
      "sample: 306\n",
      "sample: 307\n",
      "sample: 308\n",
      "sample: 309\n",
      "sample: 310\n",
      "sample: 311\n",
      "sample: 312\n",
      "sample: 313\n",
      "sample: 314\n",
      "sample: 315\n",
      "sample: 316\n",
      "sample: 317\n",
      "sample: 318\n",
      "sample: 319\n",
      "sample: 320\n",
      "sample: 321\n",
      "sample: 322\n",
      "sample: 323\n",
      "sample: 324\n",
      "sample: 325\n",
      "sample: 326\n",
      "sample: 327\n",
      "sample: 328\n",
      "sample: 329\n",
      "sample: 330\n",
      "sample: 331\n",
      "sample: 332\n",
      "sample: 333\n",
      "sample: 334\n",
      "sample: 335\n",
      "sample: 336\n",
      "sample: 337\n",
      "sample: 338\n",
      "sample: 339\n",
      "sample: 340\n",
      "sample: 341\n",
      "sample: 342\n",
      "sample: 343\n",
      "sample: 344\n",
      "sample: 345\n",
      "sample: 346\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model_moderation = get_mod_message_generation_function(ft_model, eval_tokenizer)\n",
    "test_df['fine_tuned_model_moderation'] = test_df.apply(fine_tuned_model_moderation, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013020515441894531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd3ea455e7043d6b5aa40c98357e335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 0\n",
      "sample: 1\n",
      "sample: 2\n",
      "sample: 3\n",
      "sample: 4\n",
      "sample: 5\n",
      "sample: 6\n",
      "sample: 7\n",
      "sample: 8\n",
      "sample: 9\n",
      "sample: 10\n",
      "sample: 11\n",
      "sample: 12\n",
      "sample: 13\n",
      "sample: 14\n",
      "sample: 15\n",
      "sample: 16\n",
      "sample: 17\n",
      "sample: 18\n",
      "sample: 19\n",
      "sample: 20\n",
      "sample: 21\n",
      "sample: 22\n",
      "sample: 23\n",
      "sample: 24\n",
      "sample: 25\n",
      "sample: 26\n",
      "sample: 27\n",
      "sample: 28\n",
      "sample: 29\n",
      "sample: 30\n",
      "sample: 31\n",
      "sample: 32\n",
      "sample: 33\n",
      "sample: 34\n",
      "sample: 35\n",
      "sample: 36\n",
      "sample: 37\n",
      "sample: 38\n",
      "sample: 39\n",
      "sample: 40\n",
      "sample: 41\n",
      "sample: 42\n",
      "sample: 43\n",
      "sample: 44\n",
      "sample: 45\n",
      "sample: 46\n",
      "sample: 47\n",
      "sample: 48\n",
      "sample: 49\n",
      "sample: 50\n",
      "sample: 51\n",
      "sample: 52\n",
      "sample: 53\n",
      "sample: 54\n",
      "sample: 55\n",
      "sample: 56\n",
      "sample: 57\n",
      "sample: 58\n",
      "sample: 59\n",
      "sample: 60\n",
      "sample: 61\n",
      "sample: 62\n",
      "sample: 63\n",
      "sample: 64\n",
      "sample: 65\n",
      "sample: 66\n",
      "sample: 67\n",
      "sample: 68\n",
      "sample: 69\n",
      "sample: 70\n",
      "sample: 71\n",
      "sample: 72\n",
      "sample: 73\n",
      "sample: 74\n",
      "sample: 75\n",
      "sample: 76\n",
      "sample: 77\n",
      "sample: 78\n",
      "sample: 79\n",
      "sample: 80\n",
      "sample: 81\n",
      "sample: 82\n",
      "sample: 83\n",
      "sample: 84\n",
      "sample: 85\n",
      "sample: 86\n",
      "sample: 87\n",
      "sample: 88\n",
      "sample: 89\n",
      "sample: 90\n",
      "sample: 91\n",
      "sample: 92\n",
      "sample: 93\n",
      "sample: 94\n",
      "sample: 95\n",
      "sample: 96\n",
      "sample: 97\n",
      "sample: 98\n",
      "sample: 99\n",
      "sample: 100\n",
      "sample: 101\n",
      "sample: 102\n",
      "sample: 103\n",
      "sample: 104\n",
      "sample: 105\n",
      "sample: 106\n",
      "sample: 107\n",
      "sample: 108\n",
      "sample: 109\n",
      "sample: 110\n",
      "sample: 111\n",
      "sample: 112\n",
      "sample: 113\n",
      "sample: 114\n",
      "sample: 115\n",
      "sample: 116\n",
      "sample: 117\n",
      "sample: 118\n",
      "sample: 119\n",
      "sample: 120\n",
      "sample: 121\n",
      "sample: 122\n",
      "sample: 123\n",
      "sample: 124\n",
      "sample: 125\n",
      "sample: 126\n",
      "sample: 127\n",
      "sample: 128\n",
      "sample: 129\n",
      "sample: 130\n",
      "sample: 131\n",
      "sample: 132\n",
      "sample: 133\n",
      "sample: 134\n",
      "sample: 135\n",
      "sample: 136\n",
      "sample: 137\n",
      "sample: 138\n",
      "sample: 139\n",
      "sample: 140\n",
      "sample: 141\n",
      "sample: 142\n",
      "sample: 143\n",
      "sample: 144\n",
      "sample: 145\n",
      "sample: 146\n",
      "sample: 147\n",
      "sample: 148\n",
      "sample: 149\n",
      "sample: 150\n",
      "sample: 151\n",
      "sample: 152\n",
      "sample: 153\n",
      "sample: 154\n",
      "sample: 155\n",
      "sample: 156\n",
      "sample: 157\n",
      "sample: 158\n",
      "sample: 159\n",
      "sample: 160\n",
      "sample: 161\n",
      "sample: 162\n",
      "sample: 163\n",
      "sample: 164\n",
      "sample: 165\n",
      "sample: 166\n",
      "sample: 167\n",
      "sample: 168\n",
      "sample: 169\n",
      "sample: 170\n",
      "sample: 171\n",
      "sample: 172\n",
      "sample: 173\n",
      "sample: 174\n",
      "sample: 175\n",
      "sample: 176\n",
      "sample: 177\n",
      "sample: 178\n",
      "sample: 179\n",
      "sample: 180\n",
      "sample: 181\n",
      "sample: 182\n",
      "sample: 183\n",
      "sample: 184\n",
      "sample: 185\n",
      "sample: 186\n",
      "sample: 187\n",
      "sample: 188\n",
      "sample: 189\n",
      "sample: 190\n",
      "sample: 191\n",
      "sample: 192\n",
      "sample: 193\n",
      "sample: 194\n",
      "sample: 195\n",
      "sample: 196\n",
      "sample: 197\n",
      "sample: 198\n",
      "sample: 199\n",
      "sample: 200\n",
      "sample: 201\n",
      "sample: 202\n",
      "sample: 203\n",
      "sample: 204\n",
      "sample: 205\n",
      "sample: 206\n",
      "sample: 207\n",
      "sample: 208\n",
      "sample: 209\n",
      "sample: 210\n",
      "sample: 211\n",
      "sample: 212\n",
      "sample: 213\n",
      "sample: 214\n",
      "sample: 215\n",
      "sample: 216\n",
      "sample: 217\n",
      "sample: 218\n",
      "sample: 219\n",
      "sample: 220\n",
      "sample: 221\n",
      "sample: 222\n",
      "sample: 223\n",
      "sample: 224\n",
      "sample: 225\n",
      "sample: 226\n",
      "sample: 227\n",
      "sample: 228\n",
      "sample: 229\n",
      "sample: 230\n",
      "sample: 231\n",
      "sample: 232\n",
      "sample: 233\n",
      "sample: 234\n",
      "sample: 235\n",
      "sample: 236\n",
      "sample: 237\n",
      "sample: 238\n",
      "sample: 239\n",
      "sample: 240\n",
      "sample: 241\n",
      "sample: 242\n",
      "sample: 243\n",
      "sample: 244\n",
      "sample: 245\n",
      "sample: 246\n",
      "sample: 247\n",
      "sample: 248\n",
      "sample: 249\n",
      "sample: 250\n",
      "sample: 251\n",
      "sample: 252\n",
      "sample: 253\n",
      "sample: 254\n",
      "sample: 255\n",
      "sample: 256\n",
      "sample: 257\n",
      "sample: 258\n",
      "sample: 259\n",
      "sample: 260\n",
      "sample: 261\n",
      "sample: 262\n",
      "sample: 263\n",
      "sample: 264\n",
      "sample: 265\n",
      "sample: 266\n",
      "sample: 267\n",
      "sample: 268\n",
      "sample: 269\n",
      "sample: 270\n",
      "sample: 271\n",
      "sample: 272\n",
      "sample: 273\n",
      "sample: 274\n",
      "sample: 275\n",
      "sample: 276\n",
      "sample: 277\n",
      "sample: 278\n",
      "sample: 279\n",
      "sample: 280\n",
      "sample: 281\n",
      "sample: 282\n",
      "sample: 283\n",
      "sample: 284\n",
      "sample: 285\n",
      "sample: 286\n",
      "sample: 287\n",
      "sample: 288\n",
      "sample: 289\n",
      "sample: 290\n",
      "sample: 291\n",
      "sample: 292\n",
      "sample: 293\n",
      "sample: 294\n",
      "sample: 295\n",
      "sample: 296\n",
      "sample: 297\n",
      "sample: 298\n",
      "sample: 299\n",
      "sample: 300\n",
      "sample: 301\n",
      "sample: 302\n",
      "sample: 303\n",
      "sample: 304\n",
      "sample: 305\n",
      "sample: 306\n",
      "sample: 307\n",
      "sample: 308\n",
      "sample: 309\n",
      "sample: 310\n",
      "sample: 311\n",
      "sample: 312\n",
      "sample: 313\n",
      "sample: 314\n",
      "sample: 315\n",
      "sample: 316\n",
      "sample: 317\n",
      "sample: 318\n",
      "sample: 319\n",
      "sample: 320\n",
      "sample: 321\n",
      "sample: 322\n",
      "sample: 323\n",
      "sample: 324\n",
      "sample: 325\n",
      "sample: 326\n",
      "sample: 327\n",
      "sample: 328\n",
      "sample: 329\n",
      "sample: 330\n",
      "sample: 331\n",
      "sample: 332\n",
      "sample: 333\n",
      "sample: 334\n",
      "sample: 335\n",
      "sample: 336\n",
      "sample: 337\n",
      "sample: 338\n",
      "sample: 339\n",
      "sample: 340\n",
      "sample: 341\n",
      "sample: 342\n",
      "sample: 343\n",
      "sample: 344\n",
      "sample: 345\n",
      "sample: 346\n"
     ]
    }
   ],
   "source": [
    "base_model_generation = get_mod_message_generation_function(model, tokenizer)\n",
    "test_df['base_model_moderation'] = test_df.apply(base_model_generation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[[\"index\", \"prompt\", \"label\", \"base_model_moderation\", \"fine_tuned_model_moderation\"]]\n",
    "test_df.to_csv(\"test_with_fine_tuned_moderations_mine.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
