{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3cHZaylbt2p",
        "outputId": "4b92f1ce-d149-495d-b5bc-e3fdf716b611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 1.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting httpx<1,>=0.23.0\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 7.7 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting pydantic<3,>=1.9.0\n",
            "  Downloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
            "\u001b[K     |████████████████████████████████| 395 kB 11.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting distro<2,>=1.7.0\n",
            "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: sniffio in /Users/ofiryaish/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.2.0)\n",
            "Collecting anyio<5,>=3.5.0\n",
            "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 20.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm>4 in /Users/ofiryaish/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
            "Collecting typing-extensions<5,>=4.7\n",
            "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
            "Collecting exceptiongroup>=1.0.2\n",
            "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ofiryaish/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.2)\n",
            "Collecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 24.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: certifi in /Users/ofiryaish/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2021.10.8)\n",
            "Collecting h11<0.15,>=0.13\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 18.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pydantic-core==2.16.3\n",
            "  Downloading pydantic_core-2.16.3-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 32.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: typing-extensions, h11, exceptiongroup, pydantic-core, httpcore, anyio, annotated-types, pydantic, httpx, distro, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 2.2.0\n",
            "    Uninstalling anyio-2.2.0:\n",
            "      Successfully uninstalled anyio-2.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed annotated-types-0.6.0 anyio-4.3.0 distro-1.9.0 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3 pydantic-2.6.3 pydantic-core-2.16.3 typing-extensions-4.10.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1jeaOmJnqqRL"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "client = openai.OpenAI(api_key='YOUR API TOKEN')\n",
        "\n",
        "def get_chatgpt_response(prompt):\n",
        "    \"\"\"Gets a response from ChatGPT using the OpenAI API\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # The ChatGPT model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,  # Control the creativity of the response\n",
        "        max_tokens=300  # Limit the length of the response\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgMuwR8clVYF"
      },
      "outputs": [],
      "source": [
        "from utilities import generate_branch_for_negative_tone_prompt, add_negative_branch_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP31ow77lB3x",
        "outputId": "28f471fe-d2d9-4301-bff4-b9cf9f88953b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ofiryaish/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "/Users/ofiryaish/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
            "  from pandas.core import (\n",
            "/var/folders/lh/58szb4h94wl9_p8n8htzcxgw0000gn/T/ipykernel_86923/2749749063.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n",
            "/Users/ofiryaish/Library/CloudStorage/Dropbox/cdp_data_bundle/code_to_publish/generate_branch_prompt.py:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_only_bad_tone[\"neg branch path\"] = df_only_bad_tone.apply(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "conversation_df = pd.read_csv(\"annotated_trees_101.csv\")\n",
        "conversation_df = conversation_df.rename({'Unnamed: 0': \"index\"}, axis=1)\n",
        "df_only_bad_tone = add_negative_branch_path(conversation_df)\n",
        "df_only_bad_tone = df_only_bad_tone.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are going to get an instruction, here is some reference that you might need to fulfill the instruction:\n",
            "------------------------------------------------------------------------------------\n",
            "What is CMV?\n",
            "CMV is a subreddit dedicated to civil discourse, and is built around the idea that in order to resolve our differences, we must first understand them. We believe that productive conversation requires respect and openness, and that certitude is the enemy of understanding.\n",
            "\n",
            "That's why CMV is the perfect place to post an opinion you're open to changing. We're not looking to host aggressive debates, or encourage judgement, but help each other understand different perspectives.\n",
            "\n",
            "Who can post?\n",
            "Anyone can post here so long as they have an open-mind and are looking to consider other perspectives.\n",
            "\n",
            "Here are some rules of the forum:\n",
            "Rules apply to the original poster (OP) and their submission only:\n",
            "1. Explain the reasoning behind your view, not just what that view is.\n",
            "2. You must personally hold the view and demonstrate that you are open to it changing.\n",
            "3. Posts cannot express a neutral stance, a stance regarding transgender, suggest harm against a specific person, be self-promotional, or discuss this subreddit.\n",
            "4. Only post if you are willing to have a conversation with those who reply to you.\n",
            "Rules apply to all commenters involved in the discussion:\n",
            "1. Direct responses to a submission must challenge or question at least one aspect of the submitted view. Arguments in favor of the view OP is willing to change must be restricted to replies to comments.\n",
            "2. Don't be rude or hostile to other users. Your comment will be removed even if the rest of it is solid.\n",
            "3. Refrain from accusing OP or anyone else of being unwilling to change their view.\n",
            "4. Responses must contribute meaningfully to the conversation.\n",
            "------------------------------------------------------------------------------------\n",
            "Instruction: Assume you are a forum moderator. Please respond to Hq3473, as his response might require a moderation note. Pay attention to the context and converstion subject and flow.\n",
            "\n",
            "Slagernicus: Abortion should be legal in my opinion for 4 main reasons (I understand there are more, I am just offering up 4 of my own). 1: Pregnancy’s affect on the female’s life 2: Relationship between fetus and host 3: Pregnancy circumstances 4. Overpopulation (yes, I went there) Pregnancy’s Effects on the Female’s Life: When you are pregnant, if you would like your baby to be healthy, there are plenty of things you should/shouldn’t do, which can drastically alter your lifestyle. You may have to quit a lot of your favorite hobbies. You may not be able to work your job anymore. You’ll be physically restricted in a lot of ways. This process lasts around 38 weeks. You also gain weight, which takes a while to lose after the baby is born and can cause a lot of self-consciousness issues in females directly following child birth. You will most likely have lower back pain/soreness or problems with posture once your belly begins to increase in size. Your day to day life will not be as physically involved as usual, which can affect your career and health, depending on what activities you were accustomed to performing before you got pregnant. In the gym, you are no longer supposed to live heavy weights, or perform high-impact exercises. This can be a problem if you are an avid gym goer. You can’t scuba dive, water skii, do contact sports, and it’s recommended to not eat too many seafood options due to high mercury content. Some people live as if they were going to die the next day. Those people tend to live lives full of extreme sports, fun, and danger. If you are one of those people, would you not want to be able to remove the limiting factor from your life by getting an abortion? Relationship Between Fetus and Host: In scientific terms, a parasitic relationship is classified as a relationship in which one organism, the parasite, lives in or on another organism, the host, and is harmful to it. Well, a fetus is harmful in the sense that it detracts nutrients from the daily food you eat, similar to a tapeworm. Additionally, pregnancy can cause fatigue, nausea, tender/swollen breasts, heartburn, constipation, increased stress levels, mood swings, bouts of sadness, decreased sex life, increased risk of iron deficiency anemia, increased stress on the heart, hearth rhythm issues, heart valve issues, congestive heart failure, worsening of asthma, increased risk of developing blood clots in legs if you have atrial fibrillation, back pain, severe depression (if a miscarriage occurs), anxiety, shortness of breath, leg cramps, decreased sleep, breast growth, contractions (duh), spider veins, frequent urination, vaginal discharge, food aversion/cravings, and in worst case, death. Yes, death due to pregnancy is still a real thing. \"over 600 women die each year in the United States as a result of pregnancy or delivery complications\" - the CDC (yeah yeah I know it's not really a CDC quote but i'm lazy and it was on the CDC website) Clearly, a fetus does not have a healthy impact on the body. Some pregnancy’s go off without a hitch, and are relatively pleasant in comparison to others, but the pregnancy process generally affects the body in a couple severe ways. Pregnancy Circumstances: A woman can get pregnant in many different ways. If a woman is raped, and becomes pregnant, should she be forced to keep the child? If a woman has a one night stand while using birth control, and becomes pregnant, should she be forced to keep the child? If a woman has a one night stand and the condom breaks, resulting in her pregnancy, should she be forced to keep the child? If a woman lives on her own, with absolutely no living relatives, family members, and a deceased partner, who impregnated her before his death, and her career is a career that will be lost, or severely hampered by pregnancy, should she be forced to keep the child? If a woman does not have the financial means of supporting a child, and becomes pregnant, should she be forced to keep the child? If a lesbian couple decide to have a baby, and one of the females becomes pregnant through insemination, and then they break up, or they both decide they don’t want the baby, should a baby be forced upon them? If a woman is pregnant, and is scared of the outcome of the pregnancy, or scared of death (although rare, it still happens), should she be forced to endure childbirth? If a woman is pregnant, and no longer wants the baby/regrets her decision to get pregnant, should she be forced to suffer the pains of childbirth, and the side effects of pregnancy? My answer is no to all of these. Your opinion may differ, but I believe it to be a woman’s right (and if a man could somehow get pregnant, a man’s right) to have control over her own body. If I had a tapeworm inside of me, I would want it removed. If I somehow ended up having a baby puppy inside of me, that was going to take 38 weeks to come out of me, and could potentially kill me or hurt me, I would weigh my options, look up statistics, and depending on the risk factor make a decision based on the outlook of my future and my current desires. Perhaps I would keep the puppy and life would be great, or perhaps I would not keep the puppy due to the impact it would have on my life. Either way, I would want the freedom of choice. Overpopulation: The title of this section explains itself. There are way too many people on the planet right now. Earth’s resources are limited, we should not be growing the population anymore, in fact we should probably try to slowly decrease the population to a stable and more manageable number. There is no need to explain anything here about overpopulation, this is about abortion. Overpopulation is a huge problem for the environment and our planet, and abortion is one way of reducing population growth. That is why this is one of the 4 main reasons I believe abortion should be allowed. http://www.broitsablog.com/?p=181\n",
            "\n",
            "---- Hidden part of the converstion ----\n",
            "\n",
            "Hq3473: Up until which point should abortion be legal? Should a woman be able to abort the fetus on week 36?\n",
            "\n",
            "ThatBelligerentSloth: Yes a person's bodily autonomy is absolute.\n",
            "\n",
            "Hq3473: What ahout when fetus is half way out? Can she kill it then?\n",
            "\n",
            "ThatBelligerentSloth: <quote>What ahout when fetus is half way out?</quote> <quote>Can she kill it then? </quote> Only if it is the safest and only way to preserve bodily autonomy. Essentially, she has the right to have it removed from her body in the safest way possible for her. If that ends up killing the baby, then so be it.\n",
            "\n",
            "Hq3473: So a marginal increase in safety (say 1% less chance of tearing) would justify cutting the head of the fetus that is half way born?\n",
            "\n",
            "Instruction: Assume you are a forum moderator. Please respond to Hq3473, as his response might require a moderation note. Please try not to include your opinion. Pay attention to the context and the converstion subject and flow. Be concise but also informative!\n",
            " Be kind if possible and natural like humans. Avoid messages that starts with \"Moderation Note: ...\" or \"Moderator: ...\" or other robotic like responses. Address the user you are replying to.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(generate_branch_for_negative_tone_prompt(df_only_bad_tone, conversation_df, node_index=0, with_fourm_ruls=True, context_len=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PFcGsTGNIxFp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(0, 100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [05:12<00:00,  3.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(100, 200)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [05:05<00:00,  3.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(200, 300)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [05:52<00:00,  3.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(300, 400)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:17<00:00,  1.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(400, 500)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:49<00:00,  2.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(500, 600)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [07:54<00:00,  4.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(600, 700)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:25<00:00,  2.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(700, 800)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:33<00:00,  2.73s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(800, 900)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:49<00:00,  2.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(900, 1000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:25<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1000, 1100)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:40<00:00,  2.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1100, 1200)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:32<00:00,  2.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1200, 1300)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 82/100 [04:28<03:43, 12.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16654 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 83/100 [04:56<04:52, 17.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 18066 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 84/100 [05:28<05:44, 21.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 18717 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 85/100 [06:00<06:09, 24.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19101 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 86/100 [06:30<06:08, 26.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19403 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 87/100 [07:00<05:57, 27.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19876 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 88/100 [07:31<05:39, 28.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20108 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 89/100 [07:59<05:12, 28.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20217 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 90/100 [08:27<04:42, 28.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20494 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 91/100 [08:55<04:13, 28.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20791 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 92/100 [09:22<03:42, 27.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 20956 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 93/100 [09:49<03:13, 27.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 21223 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} for i = 1293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [10:43<00:00,  6.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1300, 1400)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:58<00:00,  2.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1400, 1500)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:37<00:00,  2.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1500, 1600)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [04:44<00:00,  2.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1600, 1700)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [05:11<00:00,  3.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "range(1700, 1800)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [01:24<00:00,  2.65s/it]\n"
          ]
        }
      ],
      "source": [
        "for level in range(0, len(df_only_bad_tone)//100 + 1):\n",
        "  print(range(level * 100, (level + 1) * 100))\n",
        "  for i in tqdm(range(level * 100, min((level + 1) * 100, len(df_only_bad_tone)))):\n",
        "    faild_to_generate = True\n",
        "    context_len = None\n",
        "    while faild_to_generate:\n",
        "        try:\n",
        "            response = get_chatgpt_response(generate_branch_for_negative_tone_prompt(df_only_bad_tone, conversation_df, node_index=i, context_len=context_len))\n",
        "            faild_to_generate = False\n",
        "        except Exception as e:\n",
        "            print(\"Got\", e, \"for i =\", i)\n",
        "            if context_len is None:\n",
        "                context_len = 10\n",
        "            else:\n",
        "                context_len -= 1\n",
        "            if context_len == 1:\n",
        "                print(\"Could not load context of length 2 --> break!\")\n",
        "                break\n",
        "    df_only_bad_tone.loc[i, \"generated_moderation\"] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ro8eKP_c8ZJl"
      },
      "outputs": [],
      "source": [
        "df_only_bad_tone.to_csv(\"bad_tone_nodes_with_generated_messages_chat_gpt_3_5_turbo.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
